---
title: "p8105_hw5_xw3106"
author: Xinyu Wang(xw3106)
output: github_document
---

```{r}
library(tidyverse)
library(broom)
```

# Problem 1
```{r}
# Function: Generate n birthdays and check if duplicates exist
has_dup_birthday <- function(n, days = 365L) {
  bdays <- sample.int(days, size = n, replace = TRUE)
  
  any(duplicated(bdays))
}
```

```{r}
set.seed(8105)

n_grid <- 2:50
B <- 10000

birthday_sim_results <- map_df(n_grid, function(n) {
  
  sims <- replicate(B, has_dup_birthday(n))
  
  tibble(
    n = n,
    prob_dup = mean(sims)
  )
})
```

```{r}
# Visualization: Probability that at least two people share a birthday
ggplot(birthday_sim_results, aes(x = n, y = prob_dup)) +
  geom_line(color = "black") +
  geom_point(size = 1.4) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, 1)) +
  labs(
    title = "Birthday Paradox Simulation (10,000 trials per n)",
    x = "Group size (n)",
    y = "Pr(at least one shared birthday)"
  ) +
  theme_minimal(base_size = 12)
```

As the group size grow, we can see an increase of the probability that at least two people share a birthday.Around n = 23, this probability is approximately 50%, consistent with the well-known “birthday paradox.” As the number of people approaches 50, the probability becomes closer to 1.
This indicates that, despite having 365 possible birthdays, the probability of sharing a common birthday increases rapidly as the group size grows.

# Problem 2
```{r}
# Function: simulate dataset, return estimate and p-value
sim_t_test = function(mu, n = 30, sigma = 5) {
  
  x = rnorm(n, mean = mu, sd = sigma)
  
  test = t.test(x, mu = 0)
  
  # extract mean estimate and p-value
  tibble(
    mu_hat  = mean(x),
    p_value = broom::tidy(test)$p.value,
    mu_true = mu
  )
}

## Run simulation for mu = 0:6 with 5000 duplicates
set.seed(8105)

sim_results =
  map_df(0:6, function(mu) {
    replicate(5000, sim_t_test(mu), simplify = FALSE) |> bind_rows()
  })
```

```{r}
## Summarize simulation results
summary_df =
  sim_results |>
  group_by(mu_true) |>
  summarize(
    power= mean(p_value < 0.05),
    mean_est_all= mean(mu_hat),
    mean_est_rej= mean(mu_hat[p_value < 0.05])
  )

summary_df
```

```{r}
summary_df |>
  ggplot(aes(x = mu_true, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True mean (μ)",
    y = "Power (Pr(reject H₀))",
    title = "Power of one-sample t-test"
  ) +
  theme_bw(base_size = 12)
```

As the true mean μ increases, the statistical power also increases.
When μ = 0, the statistical power is approximately 0.05, which is consistent with the expected result of the null hypothesis.
As the true mean increases, the frequency with which the test rejects the null hypothesis H₀ also increases; when μ ≥ 5, the statistical power approaches 100%.
This pattern reflects the general relationship between effect size and statistical power—with a fixed sample size, larger effects are more easily detected.

```{r}
summary_df |>
  pivot_longer(cols = c(mean_est_all, mean_est_rej),
               names_to = "type", values_to = "mean_est") |>
  mutate(type = recode(type,
                       mean_est_all = "mean_est",
                       mean_est_rej = "mean_est_reject]")) |>
  ggplot(aes(x = mu_true, y = mean_est, color = type)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True mean (μ)",
    y = "Average estimate (μ̂)",
    title = "Bias in mean estimates across simulations"
  ) +
  theme_bw(base_size = 12)
```

The population sample mean is very close to the true mean μ, we can tell this is unbiased.
However, when considering only the sample that rejects the null hypothesis, the average estimator is consistently higher than the true mean.
This upward bias occurs because a significant result is more likely to appear in the sample when the observed mean deviates significantly from the null hypothesis. So this is a selection bias.

# Problem 3
```{r}
# Load data
homicide_df =
  read_csv("data/homicide-data.csv")
homicide_df |> 
  glimpse()
```

The dataset published by The Washington Post includes information on `r nrow(homicide_df)` homicide cases reported across `r length(unique(homicide_df$city))` major U.S. cities.
Each row represents a single homicide record, and the dataset contains `r ncol(homicide_df)` variables describing both the case and the victim.

Main variables include:

`uid`: a unique identifier for each homicide (e.g., "Alb-000001").

`reported_date`: the date the case was reported.
 
`victim_first, victim_last`: the victim’s first and last names.

`victim_race, victim_age, victim_sex`: demographic information about the victim.

`city and state`: the location where the homicide occurred.

`lat, lon`: latitude and longitude of the case.

`disposition`: the case status, showing whether it was closed by arrest, closed without arrest, or remains open.

Overall, this dataset provides detailed, case-level information that can be summarized by city to explore patterns in unsolved homicides and differences in clearance rates across metropolitan areas.

```{r}
# Create city_state and summarize
city_summary =
  homicide_df |>
  mutate(city_state = str_c(city, ", ", state)) |>
  group_by(city_state) |>
  summarize(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```

```{r}
# test for Baltimore, MD
baltimore =
  city_summary |>
  filter(city_state == "Baltimore, MD")

baltimore_test =
  prop.test(baltimore$unsolved, baltimore$total)

baltimore_tidy =
  tidy(baltimore_test) |>
  select(estimate, conf.low, conf.high)

baltimore_tidy
```

```{r}
# run prop test for all cities
city_results =
  city_summary |>
  mutate(
    test = map2(unsolved, total, ~prop.test(.x,.y)),
    tidy = map(test, tidy)
  ) |>
  unnest(tidy) |>
  select(city_state, estimate, conf.low, conf.high) |>
  arrange(desc(estimate))
```

```{r}
## Visualization
city_results |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = 0.3) +
  labs(
    x = "Estimated proportion of unsolved homicides",
    y = "City",
    title = "Proportion of unsolved homicides by city with 95% interval"
  ) +
  theme_bw(base_size = 12)
```

The estimated proportions of unsolved homicides exhibit considerable heterogeneity across the fifty metropolitan areas included in the dataset.
While the majority of cities report unsolved proportions between approximately 0.4 and 0.6, several jurisdictions deviate markedly from this range.
In particular, Chicago, IL, New Orleans, LA, and Baltimore, MD display the highest proportions of unsolved cases, approaching or exceeding 70 percent.
Conversely, Richmond, VA and Tulsa, OK show substantially lower proportions, suggesting comparatively higher clearance rates.
The observed differences are unlikely to be explained by random variation alone and may instead reflect structural disparities in investigative capacity, resource allocation, or case reporting practices among cities.